{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# importing the useful libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk import pos_tag\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import sklearn\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.hierarchy import ward, dendrogram\n",
    "from gensim import corpora, models, similarities \n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn import feature_extraction\n",
    "from sklearn.cluster import KMeans\n",
    "import pprint\n",
    "import sys\n",
    "import tempfile\n",
    "from jinja2 import Template\n",
    "from flask import Flask, render_template, flash, request, send_file\n",
    "from wtforms import Form, TextField, TextAreaField, validators, StringField, SubmitField\n",
    "from jinja2 import Environment\n",
    "from jinja2.loaders import FileSystemLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# user input html form\n",
    "template = Template('''\n",
    "<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Strict//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd\">\n",
    "<html>\n",
    "    <head>\n",
    "        <title>Identification Classification</title>\n",
    "        <link rel=\"stylesheet\" media=\"screen\" href =\"bootstrap.min.css\">\n",
    "        <meta name=\"viewport\" content = \"width=device-width, initial-scale=1.0\">\n",
    " \n",
    "    </head>\n",
    "    <body>\n",
    " \n",
    " \n",
    "<div class=\"container\">\n",
    " \n",
    " \n",
    "  <h2>Identification Classification</h2>\n",
    "    <form  action=\"http://localhost:5000/result\" method=\"POST\" role=\"form\">\n",
    "    {{ form.csrf }}\n",
    "    <div class=\"form-group\">\n",
    "      <label for=\"address\">Address:</label>\n",
    "        <input type=\"text\" class=\"form-control\" id=\"address\" name=\"address\" placeholder=\"What's your address?\">\n",
    "        <br>\n",
    "        <label for=\"algorithm\">Algorithm:</label>\n",
    "        <select name = \"algorithm\">\n",
    "        <option value=\"K_Means\">K Means</option>\n",
    "        <option value=\"lda\">LDA</option>\n",
    "        <option value=\"HCA\">HCA</option>\n",
    "        <option value=\"gmm\">GMM</option>\n",
    "        </select>\n",
    "        <br>\n",
    "        <label for=\"cluster\">Cluster:</label>\n",
    "        <input type=\"text\" class=\"form-control\" id=\"cluster\" name=\"cluster\" placeholder=\"Enter a number.\">\n",
    "        <br>\n",
    " \n",
    "    </div>\n",
    "    <button type=\"submit\" class=\"btn btn-success\">Submit</button>\n",
    "    </form>\n",
    " \n",
    "    <br>\n",
    "    \n",
    " \n",
    " </div>\n",
    "<br>            \n",
    "</div>\n",
    "</div>\n",
    "</body>\n",
    "</html>\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# result html form for image display\n",
    "template2 = Template('''\n",
    "<!doctype html>\n",
    "<html>\n",
    "   <body>\n",
    "   \n",
    "      <img src =\"C:\\Users\\44070779\\ward_clusters.png\">\n",
    "      \n",
    "   </body>\n",
    "</html>\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# result html form for cluster display\n",
    "template3 = Template('''\n",
    "<!doctype html>\n",
    "<head>\n",
    "        <title>Here are your results</title>\n",
    "</head>\n",
    "\n",
    "<html>\n",
    "    <body>\n",
    "        <table border = 1>\n",
    "         {% for line in result %}\n",
    "             <tr><td>{{ line }}</td></tr>\n",
    "            \n",
    "         {% endfor %}\n",
    "      </table>\n",
    "   </body>\n",
    "</html>\n",
    "''')\n",
    "#<td> {{ value }} </td>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# App config.\n",
    "DEBUG = True\n",
    "app = Flask(__name__)\n",
    "app.config.from_object(__name__)\n",
    "app.config['SECRET_KEY'] = '7d441f27d441f27567d441f2b6176a'\n",
    " \n",
    "class ReusableForm(Form):\n",
    "    address = TextField('Address:', validators=[validators.required()])\n",
    "    algorithm = TextField('Algorithm:', validators=[validators.required(), validators.Length(min=6, max=35)])\n",
    "    cluster = TextField('Cluster:', validators=[validators.required(), validators.Length(min=3, max=35)])\n",
    " \n",
    " \n",
    "@app.route(\"/\", methods=['GET'])\n",
    "def hello():\n",
    "    form = ReusableForm(request.form)\n",
    "    return render_template(template, form=form)\n",
    "\n",
    "@app.route('/result',methods = ['POST', 'GET'])\n",
    "def result():\n",
    "    if request.method == 'POST':\n",
    "        form = ReusableForm(request.form)\n",
    "        result = request.form\n",
    "        print form.errors\n",
    "        address=request.form['address']\n",
    "        algorithm=request.form['algorithm']\n",
    "        cluster=request.form['cluster']\n",
    "        print address, \" \", algorithm, \" \", cluster\n",
    "\n",
    "        address = address.encode('ascii','ignore')\n",
    "        algorithm = algorithm.encode('ascii','ignore')\n",
    "        cluster = int(cluster)\n",
    "        \n",
    "        #if form.validate():\n",
    "        # Save the comment here.\n",
    "         #   flash('Thanks for registration ' + algorithm)\n",
    "        #else:\n",
    "        #    flash('Error: All the form fields are required. ')\n",
    "        #func_name(address,algorithm,cluster)\n",
    "        f_names = []\n",
    "        # now we start loading the dataset\n",
    "        filenames = os.listdir(address)\n",
    "        for file in filenames:\n",
    "            f_name = os.path.join(address, file)\n",
    "            f_names.append(f_name)\n",
    "    \n",
    "        data_corpus = {}\n",
    "\n",
    "        for f in filenames:\n",
    "            f_name = os.path.join(address, f)\n",
    "            fi = open(f_name)\n",
    "            data = fi.read()\n",
    "            data_corpus[f] = data\n",
    "        data = pd.DataFrame.from_dict(data_corpus,orient='index')\n",
    "        # pandas allows u to have a sneak peak at ur data\n",
    "        data = data.reset_index()\n",
    "        data.columns = ['file_name','text']\n",
    "        \n",
    "        # cleaning the data\n",
    "        def strip(raw_text):\n",
    "            letters_only = re.sub('[\\s+]', \" \", raw_text)\n",
    "            return letters_only\n",
    "\n",
    "        data['strip']= data['text'].apply(strip)\n",
    "        \n",
    "        #removing punctuations \n",
    "        def remove_punctuation(s):\n",
    "            s = ''.join([i for i in s if i not in frozenset(string.punctuation)])\n",
    "            return s\n",
    "\n",
    "        data['text_punc'] = data['strip'].apply(remove_punctuation)\n",
    "        \n",
    "        #removing numbers\n",
    "        def numbers(raw_text):\n",
    "            letters_only = re.sub(r'\\d+', '', raw_text)\n",
    "            return letters_only\n",
    "        \n",
    "        data['num'] = data['text_punc'].apply(numbers)\n",
    "        \n",
    "        #removing non eng words\n",
    "        words = set(nltk.corpus.words.words())\n",
    "        def non_eng_text(raw_text):\n",
    "            wor = raw_text.split()\n",
    "            meaningful_words = [w for w in wor \\\n",
    "                if w.lower() in words or not w.isalpha()]\n",
    "            return( \" \".join( meaningful_words ))\n",
    "        \n",
    "        data['non_eng']= data['num'].apply(non_eng_text)\n",
    "        \n",
    "        #converting to lowercase\n",
    "        data['lower'] = data['non_eng'].str.lower()\n",
    "        \n",
    "        #removing stopwords\n",
    "        def clean_text(raw_text):\n",
    "            letters_only = re.sub(\"[^a-z]\", \" \", raw_text)\n",
    "            words = letters_only.split()\n",
    "            corp = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours',\n",
    "        'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers',\n",
    "        'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves',\n",
    "        'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are',\n",
    "        'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does',\n",
    "        'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until',\n",
    "        'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into',\n",
    "        'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down',\n",
    "        'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here',\n",
    "        'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\n",
    "        'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so',\n",
    "        'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now']\n",
    "            stops = set(corp)\n",
    "            meaningful_words = [w for w in words if not w in stops and len(w)>1]\n",
    "            return( \" \".join( meaningful_words ))\n",
    "        \n",
    "        data['stop']= data['lower'].apply(clean_text)\n",
    "        \n",
    "        #lemmatizing the words\n",
    "        def lemma(raw_text):\n",
    "            wor = raw_text.split()\n",
    "            wnl = WordNetLemmatizer()\n",
    "            meaningful = [wnl.lemmatize(i,j[0].lower()) if j[0].lower() in ['a','n','v'] else wnl.lemmatize(i) for i,j in pos_tag(wor)]\n",
    "            return ( \" \".join( meaningful))\n",
    "        \n",
    "        data['text_lemma']= data['stop'].apply(lemma)\n",
    "        \n",
    "        #creating tf-idf of the dataset available\n",
    "        corp = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours',\n",
    "        'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers',\n",
    "        'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves',\n",
    "        'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are',\n",
    "        'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does',\n",
    "        'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until',\n",
    "        'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into',\n",
    "        'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down',\n",
    "        'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here',\n",
    "        'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\n",
    "        'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so',\n",
    "        'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now']\n",
    "        tfidf_vectorizer = TfidfVectorizer(max_features=500000, stop_words = corp, use_idf= True, ngram_range=(1,3))\n",
    "        train_data_feature=tfidf_vectorizer.fit_transform(data['text_lemma'])\n",
    "        tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "        tf_vectorizer = CountVectorizer( max_features=500000, stop_words = corp)\n",
    "        tf = tf_vectorizer.fit_transform(data['text_lemma'])\n",
    "        tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "        train_data_norm = sklearn.preprocessing.normalize(train_data_feature)\n",
    "        tf_norm = sklearn.preprocessing.normalize(tf)\n",
    "        train_data_np = train_data_norm.toarray()\n",
    "        tf_np = tf_norm.toarray()\n",
    "        dist = 1 - cosine_similarity(train_data_feature)\n",
    "        \n",
    "        #checking which algorithm user has asked for\n",
    "        if(algorithm == \"HCA\"):\n",
    "            #titles of the document in the dataset\n",
    "            titles = open('NAME.txt').read().split('\\n')\n",
    "            linkage_matrix = ward(dist) #define the linkage_matrix using ward clustering pre-computed distances\n",
    "\n",
    "            fig, ax = plt.subplots(figsize=(15, 20)) # set size\n",
    "            ax = dendrogram(linkage_matrix, orientation=\"right\", labels=titles); \n",
    "            #response= send_file(io.BytesIO(f.read()), minetype='image/png')\n",
    "            \n",
    "            plt.tick_params(\\\n",
    "            axis= 'x',          # changes apply to the x-axis\n",
    "            which='both',      # both major and minor ticks are affected\n",
    "            bottom='off',      # ticks along the bottom edge are off\n",
    "            top='off',         # ticks along the top edge are off\n",
    "            labelbottom='off')\n",
    "\n",
    "            plt.tight_layout() #show plot with tight layout\n",
    "            plt.savefig('ward_clusters.png', dpi=200)#save figure as ward_clusters\n",
    "            return render_template(template2)\n",
    "        elif(algorithm == \"LDA\"): \n",
    "                def strip_proppers(text):\n",
    "            # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "                    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent) if word.islower()]\n",
    "                    return \"\".join([\" \"+i if not i.startswith(\"'\") and i not in string.punctuation else i for i in tokens]).strip()\n",
    "                def strip_proppers_POS(text):\n",
    "                    tagged = pos_tag(text.split()) #use NLTK's part of speech tagger\n",
    "                    non_propernouns = [word for word,pos in tagged if pos != 'NNP' and pos != 'NNPS']\n",
    "                    return non_propernouns\n",
    "                stopwords = nltk.corpus.stopwords.words('english')\n",
    "                stemmer = SnowballStemmer(\"english\")\n",
    "                def tokenize_and_stem(text):\n",
    "                    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "                    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "                    filtered_tokens = []\n",
    "                    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "                    for token in tokens:\n",
    "                        if re.search('[a-zA-Z]', token):\n",
    "                            filtered_tokens.append(token)\n",
    "                    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "                    return stems\n",
    "                preprocess = [strip_proppers(doc) for doc in data['stop']]\n",
    "\n",
    "                tokenized_text = [tokenize_and_stem(text) for text in preprocess]\n",
    "\n",
    "                texts = [[word for word in text if word not in stopwords] for text in tokenized_text]\n",
    "                texts = [[word for word in text] for text in texts]\n",
    "\n",
    "                dictionary = corpora.Dictionary(texts)\n",
    "                corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "                lda = models.LdaModel(corpus, num_topics=cluster, id2word=dictionary, update_every=5, chunksize=10000, passes=100)\n",
    "                topics = lda.print_topics(cluster, num_words=20)\n",
    "                topics_matrix = lda.show_topics(formatted=False, num_words=20)\n",
    "                topics_matrix_1 = np.array(topics_matrix, dtype=object)\n",
    "                topic_words = topics_matrix_1[:,1]\n",
    "                lda_corpus = lda[mm]\n",
    "                #compute threshold \n",
    "                scores = list(chain(*[[score for topic_id,score in topic] \\\n",
    "                          for topic in [doc for doc in lda_corpus]]))\n",
    "                threshold = sum(scores)/len(scores)\n",
    "                lda_corpus = [max(prob,key=lambda y:y[1]) for prob in lda[mm] ]\n",
    "                playlists = [[] for i in xrange(5)]\n",
    "                a = []\n",
    "                #returning top 20 words for each cluster formed\n",
    "                for i in topic_words:\n",
    "                    b = \"\"\n",
    "                    for word in i:\n",
    "                        b = b + str(word) + \", \"  \n",
    "                    a.append(b)\n",
    "                print a\n",
    "                return render_template(template3, result=a)\n",
    "                #for i, x in enumerate(lda_corpus):\n",
    "                    #playlists[x[0]].append(data['file_name'][i])\n",
    "                #for idx in range (cluster):\n",
    "                    ##newpath = ((r'C:\\Users\\44070779\\Desktop\\folder_%s') % (idx)) \n",
    "                    #if not os.path.exists(newpath): os.makedirs(newpath)\n",
    "        elif(algorithm == \"K_Means\"):\n",
    "            print \"K-Means\"\n",
    "            km =  KMeans(n_clusters=cluster)\n",
    "            km.fit(train_data_feature)\n",
    "            clusters = km.labels_.tolist()\n",
    "            data['cluster'] = clusters\n",
    "            km_model = []\n",
    "            '''for i in xrange(2,8):\n",
    "                print (\"Running Kmeans with\", i , \"clusters\")\n",
    "                km_model.append(KMeans(n_clusters=i).fit(train_data_norm))\n",
    "            def print_file_with_clusters(cluster):\n",
    "                file_with_clusters = {}\n",
    "                data['cluster'] = km_model[cluster - 2].labels_\n",
    "\n",
    "                for i in xrange(cluster):\n",
    "                    file_with_clusters[i] = list(data[data['cluster']==i]['file_name'])\n",
    "\n",
    "                pprint.pprint(file_with_clusters)'''\n",
    "            order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "            a = []\n",
    "            #returning top 20 words for each cluster formed\n",
    "            for i in range(cluster):\n",
    "                b = \"\"\n",
    "                for ind in order_centroids[i, :20]:\n",
    "                    b = b + tfidf_feature_names[ind] + \", \"  \n",
    "                a.append(b)\n",
    "            print a\n",
    "            return render_template(template3, result=a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#making the app run\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=False)\n",
    "#wait for the magic ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
